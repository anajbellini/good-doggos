{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Doggos: Wrangling Process\n",
    "_by **Ana Júlia Bellini**_\n",
    "\n",
    "_10 Dec 2021_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to put students’ data wrangling skills to the test by cleaning three datasets on @WeRateDogs – a Twitter page – tweets spanning from X to Y. All of the data comes from a variety of sources, such as APIs and external servers.\n",
    "\n",
    "Throughout the following sections, we’ll bring details on the issues found and how each wrangling process step was conducted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final dataset contains information extracted from three sources, listed below:\n",
    "- **twitter-archive-enhanced.csv** – a CSV file with the Tweet content, ratings, stages, timestamps, and other information; imported from a local file.\n",
    "- **Twitter API** – using my particular credentials, we’ve gathered favourite and retweet counts for each Tweet contained in the previous CSV file.\n",
    "- **image-predictions.tsv** – a TSV file containing predictions over each dog’s breed, coming from an AI algorithm. Imported from Udacity’s server via an HTTP request.\n",
    "\n",
    "To gather this data, the following libraries were used:\n",
    "- pandas;\n",
    "- numpy;\n",
    "- json;\n",
    "- io;\n",
    "- re;\n",
    "- tweepy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identified Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After programmatic and visual assessment performed mainly with Pandas and Numpy, there were **10 quality issues** and **2 tidiness issues** found in the data, described as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A considerable number of columns that don't bring much value to our analyses (e.g. `expanded_url` in Twitter archive);\n",
    "2. There are some Retweets among the posts that must be dropped, as per Udacity's request;\n",
    "3. There are also Tweets older than August 1st, 2017 that are to be dropped as well;\n",
    "4. Breed names start with uppercase and its words are separated by underscores, instead of actual white spaces;\n",
    "5. Not all Tweets from the main archive are in the other two files, since some of them seem to have been deleted;\n",
    "6. Rows with invalid column names (e.g. \"a\", \"the\");\n",
    "7. Rows with ratings not conforming to the standard system of some score out of 10 (e.g. 24/7);\n",
    "8. Some Tweets mention two dog stages;\n",
    "9. Columns with the wrong datatype (e.g. `timestamp` should be `datetime`).\n",
    "10. Dog breed information could be within any of three different columns, depending on the prediction's confidence level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The \"dog stage\" information (doggo, pupper, puppo, floofer) is spread across four other columns, when it's part of a single categorical variable, violating the \"each variable forms a column\" rule;\n",
    "2. We currently have three different tables containing information on Tweets, when it should be all part of a single set, not abiding by the \"each type of observational unit forms a table\" rule;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the aforementioned issues could be solved by **filtering the DataFrame based on certain complex conditions**, along with some special Pandas methods, such as:\n",
    "\n",
    "- `cumsum`;\n",
    "- `agg`;\n",
    "- `get_loc`;\n",
    "- `apply`;\n",
    "- `to_datetime`;\n",
    "\n",
    "And many others.\n",
    "\n",
    "To get a better idea of what we're working on when treating each issue, the Define-Code-Test framework was used.\n",
    "\n",
    "After performing all cleaning steps, we were left with a master dataset made of the following columns:\n",
    "\n",
    "- `tweet_id`;\n",
    "- `timestamp`;\n",
    "- `text`;\n",
    "- `rating_numerator`;\n",
    "- `rating_denominator`;\n",
    "- `name`;\n",
    "- `retweet_count`;\n",
    "- `favorite_count`;\n",
    "- `stage`;\n",
    "- `breed`."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c62b783a36d4a54d04a71e7ccd8349d403cc9894e3f1b475fafe269c56e2d3b7"
  },
  "kernelspec": {
   "display_name": "Python [conda env:good-doggo]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
